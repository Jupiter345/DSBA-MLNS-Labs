{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Graph Neural Networks (GNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from dgl.nn import GraphConv\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Node Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 8\n",
      "Number of nodes: 7650\n",
      "Number of edges: 245813\n"
     ]
    }
   ],
   "source": [
    "# Import datasat\n",
    "dataset = dgl.data.AmazonCoBuyPhotoDataset()\n",
    "print('Number of classes:', dataset.num_classes)\n",
    "\n",
    "# A DGL Dataset object may contain one or multiple graphs. The Amazon\n",
    "# dataset used in this lab only consists of one single graph.\n",
    "graph = dataset[0]\n",
    "graph = dgl.add_self_loop(graph)\n",
    "\n",
    "print('Number of nodes:', graph.num_nodes())\n",
    "print('Number of edges:', graph.num_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DGL graph can store node features in a\n",
    "dictionary-like attribute called ``ndata``.\n",
    "In the DGL Amazon co-buy dataset, the graph contains the following node features:\n",
    "\n",
    "- ``label``: The ground truth node category.\n",
    "\n",
    "-  ``feat``: The node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node labels and features\n",
      "{'label': tensor([2, 2, 2,  ..., 3, 1, 1]), 'feat': tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [1., 1., 0.,  ..., 1., 0., 1.]])}\n"
     ]
    }
   ],
   "source": [
    "print('Node labels and features')\n",
    "print(graph.ndata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Retrieve key properties of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key graph variables\n",
    "X = None\n",
    "y = None \n",
    "num_classes = None\n",
    "num_feat = None\n",
    "N = None\n",
    "\n",
    "print('Number of features: ', num_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Split into train/validation/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(N, train_ratio, seed=4):\n",
    "    \"\"\" Creates train/val/test masks\n",
    "\n",
    "    Args:\n",
    "        N (int): dataset size\n",
    "        train_ratio (float): proportion of the training set\n",
    "        seed (int, optional): Fixes random. Defaults to 10\n",
    "\n",
    "    Return: \n",
    "        [tensors]: returns boolean tensors for train/val/test set\n",
    "        True indicates that a node belong to this set, False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    train_size = int(train_ratio * N)\n",
    "    val_size = int((N - train_size)/2)\n",
    "    test_size = N - train_size - val_size\n",
    "\n",
    "    # split dataset\n",
    "    subsets = torch.utils.data.random_split(range(N), lengths = [train_size, val_size, test_size], generator=torch.Generator().manual_seed(seed))\n",
    "    train_inds, val_inds, test_inds = [torch.Tensor(subset.indices) for subset in subsets]\n",
    "\n",
    "    # create tensors of masks for each subset\n",
    "    dataset_inds = torch.arange(N)\n",
    "    train_mask = torch.isin(dataset_inds, train_inds)\n",
    "    val_mask = torch.isin(dataset_inds, val_inds)\n",
    "    test_mask = torch.isin(dataset_inds, test_inds)\n",
    "\n",
    "    return train_mask, val_mask, test_mask\n",
    "\n",
    "train_mask, val_mask, test_mask = split_dataset(N, train_ratio=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Implement a Graph Convolutional Network\n",
    "<center><img src=\"./gcn_web.png\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_model(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Define a Graph Convolutional Network \n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, input_size, hidden_size, output_size, dropout):\n",
    "        super(GNN_model, self).__init__()\n",
    "        # Fill in \n",
    "        # Define GNN components\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        # Fill in\n",
    "        # Implement the forward function that takes the graph,\n",
    "        # the features tensor x and returns the output tensor as shown in figure 1\n",
    "\n",
    "        return # output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, graph, x, labels, num_epochs, optimizer, train_mask, val_mask, test_mask):\n",
    "    \"\"\" Train the GNN model \n",
    "\n",
    "    Args:\n",
    "        model: GNN model defined in pytorch\n",
    "        graph (dgl.graph): dataset on which the task is performed\n",
    "        x (tensor): node feature matrix \n",
    "        labels (tensor): node labels\n",
    "        num_epochs (int): number of epochs\n",
    "        optimizer: Adam optimizer\n",
    "        train_mask (tensor): boolean mask for training nodes\n",
    "        val_mask (tensor): boolean mask for validation set\n",
    "    \"\"\"\n",
    "    \n",
    "    # Train the model (pytorch specific)\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass -- compute predictions\n",
    "        pred = #\n",
    "\n",
    "        # Compute loss\n",
    "        # Note that you should only compute the losses of the nodes in the training set.\n",
    "        loss = #\n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward\n",
    "        # Fill in the blank\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch {}: loss {:.3f}, train Acc: {:.3f}, val acc: {:.3f}, test acc: {:.3f}'.format(\n",
    "                epoch, loss, train_acc, val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate model\n",
    "num_layers=3\n",
    "hidden_size=16\n",
    "dropout=0\n",
    "num_epochs=400\n",
    "lr=0.01,\n",
    "weight_decay=0.005,\n",
    "train_ratio=0.8,\n",
    "seed=4\n",
    "\n",
    "model = #\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = #\n",
    "\n",
    "# Train model\n",
    "train(model, graph, X, y, num_epochs, optimizer, train_mask, val_mask, test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Graph Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = #\n",
    "\n",
    "# Add self loop to each graph\n",
    "for i, graph in enumerate(dataset):\n",
    "    graph, label = graph\n",
    "    graph = #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train, validation and test sets\n",
    "train_sampler, val_sampler, test_sampler = #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch graphs with GraphDataLoader\n",
    "train_dataloader = #\n",
    "val_dataloader = #\n",
    "test_dataloader = #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Create GNN model for graph classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicGraphModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_layers, input_size, hidden_size, output_size):\n",
    "        super(BasicGraphModel, self).__init__()\n",
    "\n",
    "        # Define GNN components\n",
    "\n",
    "    def forward(self, g, x):\n",
    "\n",
    "        # Create GNN \n",
    "\n",
    "        return # output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fcn, optimizer, train_dataloader, val_dataloader):\n",
    "    # Train model\n",
    "    # Fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loss_fcn, dataloader):\n",
    "    # Test predictions\n",
    "    # Fill in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, subgraph, labels, loss_fcn):\n",
    "    # Evaluate model\n",
    "    # Fill in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store features\n",
    "n_features, n_classes = dataset[0][0].ndata['node_attr'].shape[1], \\\n",
    "    dataset.num_labels[0]\n",
    "\n",
    "# Define model, loss function and optimizer\n",
    "model = # \n",
    "optimizer = # \n",
    "loss_fcn = # \n",
    "\n",
    "# Train and test\n",
    "train(model, loss_fcn, optimizer, train_dataloader, val_dataloader)\n",
    "test(model, loss_fcn, test_dataloader)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb33d593d47f651fba572ce8ac90e48dbb0d0a1a149f2b36453d2060340ffeb2"
  },
  "kernelspec": {
   "display_name": "mlns",
   "language": "python",
   "name": "mlns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
