{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Graph Neural Networks (GNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from dgl.nn import GraphConv\n",
    "from IPython.display import Latex\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Node Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 8\n",
      "Number of nodes: 7650\n",
      "Number of edges: 245813\n"
     ]
    }
   ],
   "source": [
    "# Import datasat\n",
    "dataset = dgl.data.AmazonCoBuyPhotoDataset()\n",
    "print('Number of classes:', dataset.num_classes)\n",
    "\n",
    "# A DGL Dataset object may contain one or multiple graphs. The Amazon\n",
    "# dataset used in this lab only consists of one single graph.\n",
    "graph = dataset[0]\n",
    "graph = dgl.add_self_loop(graph)\n",
    "\n",
    "print('Number of nodes:', graph.num_nodes())\n",
    "print('Number of edges:', graph.num_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DGL graph can store node features in a\n",
    "dictionary-like attribute called ``ndata``.\n",
    "In the DGL Amazon co-buy dataset, the graph contains the following node features:\n",
    "\n",
    "- ``label``: The ground truth node category.\n",
    "\n",
    "-  ``feat``: The node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node labels and features\n",
      "{'label': tensor([6, 4, 3,  ..., 1, 2, 3]), 'feat': tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])}\n"
     ]
    }
   ],
   "source": [
    "print('Node labels and features')\n",
    "print(graph.ndata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Retrieve key properties of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  745\n"
     ]
    }
   ],
   "source": [
    "# Define key graph variables\n",
    "X = graph.ndata['feat']\n",
    "y = graph.ndata['label']\n",
    "num_classes = dataset.num_classes\n",
    "num_feat = X.shape[1]\n",
    "N = graph.number_of_nodes()\n",
    "\n",
    "print('Number of features: ', num_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(N, train_ratio, seed=4):\n",
    "    \"\"\" Creates train/val/test masks\n",
    "\n",
    "    Args:\n",
    "        N (int): dataset size\n",
    "        train_ratio (float): proportion of the training set\n",
    "        seed (int, optional): Fixes random. Defaults to 10\n",
    "\n",
    "    Return: \n",
    "        [tensors]: returns boolean tensors for train/val/test set\n",
    "        True indicates that a node belong to this set, False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    train_size = int(train_ratio * N)\n",
    "    val_size = int((N - train_size)/2)\n",
    "    test_size = N - train_size - val_size\n",
    "\n",
    "    # split dataset\n",
    "    subsets = torch.utils.data.random_split(range(N), lengths = [train_size, val_size, test_size], generator=torch.Generator().manual_seed(seed))\n",
    "    train_inds, val_inds, test_inds = [torch.Tensor(subset.indices) for subset in subsets]\n",
    "\n",
    "    # create tensors of masks for each subset\n",
    "    dataset_inds = torch.arange(N)\n",
    "    train_mask = torch.isin(dataset_inds, train_inds)\n",
    "    val_mask = torch.isin(dataset_inds, val_inds)\n",
    "    test_mask = torch.isin(dataset_inds, test_inds)\n",
    "\n",
    "    return train_mask, val_mask, test_mask\n",
    "\n",
    "train_mask, val_mask, test_mask = split_dataset(N, train_ratio=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Implement a Graph Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H^{(l+1)} = f(H^{(l)}, A) = \\sigma( \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./gcn_web.png\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_model(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Define a Graph Convolution Network \n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, input_size, hidden_size, output_size, dropout):\n",
    "        super(GNN_model, self).__init__()\n",
    "\n",
    "        # Define GNN components\n",
    "        self.convs = torch.nn.ModuleList() # holds GraphConv layers in a list\n",
    "        self.convs.append(\n",
    "            GraphConv(input_size, hidden_size, activation=F.relu)) # You can either define the activation at the layer level or call it inside the forward\n",
    "        for i in range(num_layers-2):\n",
    "            self.convs.append(\n",
    "                GraphConv(hidden_size, hidden_size, activation=F.relu))\n",
    "        self.convs.append(GraphConv(hidden_size, output_size))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, graph, x):\n",
    "        # Implement the forward function that takes the graph,\n",
    "        # the features tensor x and returns the output tensor as shown in figure 1\n",
    "        for conv in self.convs:\n",
    "            x = conv(graph, x)\n",
    "    \n",
    "        output = F.log_softmax(x, dim=1) # Log_softmax is more stable numerically in comparison to softmax\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, graph, x, labels, num_epochs, optimizer, train_mask, val_mask, test_mask):\n",
    "    \"\"\" Train the GNN model \n",
    "\n",
    "    Args:\n",
    "        model: GNN model defined in pytorch\n",
    "        graph (dgl.graph): dataset on which the task is performed\n",
    "        x (tensor): node feature matrix \n",
    "        labels (tensor): node labels\n",
    "        num_epochs (int): number of epochs\n",
    "        optimizer: Adam optimizer\n",
    "        train_mask (tensor): boolean mask for training nodes\n",
    "        val_mask (tensor): boolean mask for validation set\n",
    "    \"\"\"\n",
    "    \n",
    "    # Train the model (pytorch specific)\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "    nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward\n",
    "        pred = model(graph, x)\n",
    "\n",
    "        # Compute loss\n",
    "        # Note that you should only compute the losses of the nodes in the training set.\n",
    "        loss = nll_loss(pred[train_mask], labels[train_mask])\n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch {}: loss {:.3f}, train Acc: {:.3f}, val acc: {:.3f}, test acc: {:.3f}'.format(\n",
    "                epoch, loss, train_acc, val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate model\n",
    "num_layers=3\n",
    "hidden_size=16\n",
    "dropout=0.3\n",
    "num_epochs=300\n",
    "lr=0.01\n",
    "weight_decay=0.005\n",
    "train_ratio=0.8\n",
    "seed=4\n",
    "\n",
    "model = GNN_model(num_layers, num_feat, hidden_size, num_classes, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 2.059, train Acc: 0.221, val acc: 0.197, test acc: 0.200\n",
      "Epoch 10: loss 1.529, train Acc: 0.464, val acc: 0.456, test acc: 0.444\n",
      "Epoch 20: loss 1.072, train Acc: 0.708, val acc: 0.720, test acc: 0.697\n",
      "Epoch 30: loss 0.687, train Acc: 0.817, val acc: 0.817, test acc: 0.807\n",
      "Epoch 40: loss 0.500, train Acc: 0.880, val acc: 0.865, test acc: 0.869\n",
      "Epoch 50: loss 0.394, train Acc: 0.908, val acc: 0.884, test acc: 0.905\n",
      "Epoch 60: loss 0.352, train Acc: 0.908, val acc: 0.897, test acc: 0.908\n",
      "Epoch 70: loss 0.325, train Acc: 0.916, val acc: 0.903, test acc: 0.907\n",
      "Epoch 80: loss 0.312, train Acc: 0.919, val acc: 0.908, test acc: 0.908\n",
      "Epoch 90: loss 0.298, train Acc: 0.926, val acc: 0.895, test acc: 0.912\n",
      "Epoch 100: loss 0.303, train Acc: 0.921, val acc: 0.894, test acc: 0.918\n",
      "Epoch 110: loss 0.303, train Acc: 0.918, val acc: 0.890, test acc: 0.906\n",
      "Epoch 120: loss 0.284, train Acc: 0.927, val acc: 0.907, test acc: 0.916\n",
      "Epoch 130: loss 0.276, train Acc: 0.930, val acc: 0.907, test acc: 0.920\n",
      "Epoch 140: loss 0.272, train Acc: 0.932, val acc: 0.899, test acc: 0.915\n",
      "Epoch 150: loss 0.267, train Acc: 0.933, val acc: 0.908, test acc: 0.923\n",
      "Epoch 160: loss 0.261, train Acc: 0.934, val acc: 0.911, test acc: 0.919\n",
      "Epoch 170: loss 0.255, train Acc: 0.936, val acc: 0.912, test acc: 0.922\n",
      "Epoch 180: loss 0.250, train Acc: 0.937, val acc: 0.919, test acc: 0.922\n",
      "Epoch 190: loss 0.291, train Acc: 0.912, val acc: 0.901, test acc: 0.902\n",
      "Epoch 200: loss 0.244, train Acc: 0.939, val acc: 0.920, test acc: 0.928\n",
      "Epoch 210: loss 0.257, train Acc: 0.928, val acc: 0.901, test acc: 0.915\n",
      "Epoch 220: loss 0.544, train Acc: 0.868, val acc: 0.852, test acc: 0.863\n",
      "Epoch 230: loss 0.385, train Acc: 0.878, val acc: 0.868, test acc: 0.867\n",
      "Epoch 240: loss 0.286, train Acc: 0.926, val acc: 0.897, test acc: 0.918\n",
      "Epoch 250: loss 0.267, train Acc: 0.931, val acc: 0.906, test acc: 0.922\n",
      "Epoch 260: loss 0.245, train Acc: 0.937, val acc: 0.915, test acc: 0.924\n",
      "Epoch 270: loss 0.240, train Acc: 0.936, val acc: 0.920, test acc: 0.920\n",
      "Epoch 280: loss 0.238, train Acc: 0.939, val acc: 0.918, test acc: 0.928\n",
      "Epoch 290: loss 0.237, train Acc: 0.937, val acc: 0.918, test acc: 0.924\n"
     ]
    }
   ],
   "source": [
    "# Define an optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=weight_decay)\n",
    "\n",
    "# Train model\n",
    "train(model, graph, X, y, num_epochs, optimizer, train_mask, val_mask, test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Graph Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dgl.data.TUDataset(name='ENZYMES')\n",
    "\n",
    "# Add self loop to each graph\n",
    "dataset.graph_lists = [dgl.add_self_loop(graph) for graph in dataset.graph_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Graph(num_nodes=37, num_edges=205,\n",
       "       ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), 'node_attr': Scheme(shape=(18,), dtype=torch.int64), 'node_labels': Scheme(shape=(1,), dtype=torch.int64)}\n",
       "       edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}),\n",
       " tensor([5]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graph categories: [6]\n",
      "Dimension of nodes features 18\n"
     ]
    }
   ],
   "source": [
    "print('Number of graph categories:', dataset.num_labels)\n",
    "print('Dimension of nodes features', dataset[0][0].ndata['node_attr'].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train, validation and test sets\n",
    "train_sampler, val_sampler, test_sampler = dgl.data.utils.split_dataset(\n",
    "        dataset, frac_list=[0.6, 0.2, 0.2], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch graphs with GraphDataLoader\n",
    "train_dataloader = GraphDataLoader(\n",
    "        train_sampler, batch_size=5, drop_last=False)\n",
    "val_dataloader = GraphDataLoader(\n",
    "    val_sampler, batch_size=5, drop_last=False)\n",
    "test_dataloader = GraphDataLoader(\n",
    "    test_sampler, batch_size=5, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Create GNN model for graph classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicGraphModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_layers, input_size, hidden_size, output_size):\n",
    "        super(BasicGraphModel, self).__init__()\n",
    "\n",
    "        # Define GNN components\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GraphConv(input_size, hidden_size))\n",
    "        for i in range(n_layers-1):\n",
    "            self.convs.append(GraphConv(hidden_size, hidden_size))\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        # Message Passing -- Learn node representations via GCN\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(g, x)\n",
    "            x = F.elu(x)\n",
    "        x = self.convs[-1](g, x)\n",
    "        # Readout -- average all node representations to get graph embedding\n",
    "        g.ndata['h'] = x\n",
    "        x = dgl.mean_nodes(g, 'h')\n",
    "        # Apply linear layer to classify graph representation\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fcn, optimizer, train_dataloader, val_dataloader, num_epochs):\n",
    "    model = model.double()\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        for batch, batched_graph in enumerate(train_dataloader):\n",
    "            batched_graph, labels = batched_graph\n",
    "            logits = model(batched_graph, batched_graph.ndata['node_attr'].double())\n",
    "            loss = loss_fcn(logits, labels.T[0])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        loss_data = np.mean(losses)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch {} | Loss: {:.4f}\".format(epoch, loss_data))\n",
    "            test(model, loss_fcn, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loss_fcn, dataloader):\n",
    "    scores = []\n",
    "    for batch, batched_graph in enumerate(dataloader):\n",
    "        batched_graph, labels = batched_graph\n",
    "        scores.append(\n",
    "            evaluate(model, batched_graph, labels, loss_fcn))\n",
    "    mean_scores = np.mean(scores)\n",
    "    print(\"Accuracy score: {:.4f}\".format(mean_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, batched_graph, labels, loss_fcn):\n",
    "    model = model.double()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(batched_graph, batched_graph.ndata['node_attr'].double())\n",
    "\n",
    "    labels = labels.T[0]\n",
    "    loss = loss_fcn(output, labels)\n",
    "    predict = output.argmax(dim=1)\n",
    "    score = (labels == predict).sum().item() / len(labels)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 1.9696\n",
      "Accuracy score: 0.2167\n",
      "Epoch 5 | Loss: 1.6956\n",
      "Accuracy score: 0.2500\n",
      "Epoch 10 | Loss: 1.6642\n",
      "Accuracy score: 0.2583\n",
      "Epoch 15 | Loss: 1.6076\n",
      "Accuracy score: 0.2750\n",
      "Epoch 20 | Loss: 1.5372\n",
      "Accuracy score: 0.2833\n",
      "Epoch 25 | Loss: 1.4642\n",
      "Accuracy score: 0.3000\n",
      "Epoch 30 | Loss: 1.3809\n",
      "Accuracy score: 0.3583\n",
      "Epoch 35 | Loss: 1.2993\n",
      "Accuracy score: 0.3500\n",
      "Epoch 40 | Loss: 1.2195\n",
      "Accuracy score: 0.3417\n",
      "Epoch 45 | Loss: 1.1524\n",
      "Accuracy score: 0.3250\n",
      "Epoch 50 | Loss: 1.0951\n",
      "Accuracy score: 0.3667\n",
      "Epoch 55 | Loss: 1.0476\n",
      "Accuracy score: 0.3500\n",
      "Epoch 60 | Loss: 1.0443\n",
      "Accuracy score: 0.3417\n",
      "Epoch 65 | Loss: 0.9964\n",
      "Accuracy score: 0.3583\n",
      "Epoch 70 | Loss: 0.9833\n",
      "Accuracy score: 0.3583\n",
      "Epoch 75 | Loss: 0.9031\n",
      "Accuracy score: 0.3833\n",
      "Epoch 80 | Loss: 0.8359\n",
      "Accuracy score: 0.4083\n",
      "Epoch 85 | Loss: 0.7944\n",
      "Accuracy score: 0.4167\n",
      "Epoch 90 | Loss: 0.7931\n",
      "Accuracy score: 0.4500\n",
      "Epoch 95 | Loss: 0.7201\n",
      "Accuracy score: 0.4083\n",
      "Epoch 100 | Loss: 0.7284\n",
      "Accuracy score: 0.4667\n",
      "Epoch 105 | Loss: 0.5965\n",
      "Accuracy score: 0.4417\n",
      "Epoch 110 | Loss: 0.6141\n",
      "Accuracy score: 0.4000\n",
      "Epoch 115 | Loss: 0.6302\n",
      "Accuracy score: 0.4417\n",
      "Epoch 120 | Loss: 0.7240\n",
      "Accuracy score: 0.4583\n",
      "Epoch 125 | Loss: 0.4725\n",
      "Accuracy score: 0.4667\n",
      "Epoch 130 | Loss: 0.5085\n",
      "Accuracy score: 0.4917\n",
      "Epoch 135 | Loss: 0.8133\n",
      "Accuracy score: 0.4500\n",
      "Epoch 140 | Loss: 0.3891\n",
      "Accuracy score: 0.4917\n",
      "Epoch 145 | Loss: 0.5097\n",
      "Accuracy score: 0.4667\n",
      "Accuracy score: 0.4583\n"
     ]
    }
   ],
   "source": [
    "# Store features\n",
    "n_features, n_classes = dataset[0][0].ndata['node_attr'].shape[1], \\\n",
    "    dataset.num_labels[0]\n",
    "hidden_size = 64\n",
    "\n",
    "# Define model, loss function and optimizer\n",
    "model = BasicGraphModel(n_layers=3, input_size=n_features,\n",
    "                        hidden_size=hidden_size, output_size=n_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and test\n",
    "train(model, loss_fcn, optimizer,\n",
    "        train_dataloader, val_dataloader, num_epochs=150)\n",
    "test(model, loss_fcn, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlns",
   "language": "python",
   "name": "mlns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
